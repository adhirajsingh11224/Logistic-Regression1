#all important imports
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
plt.style.use("ggplot")
%matplotlib inline

#setting the standard figure size
from pylab import rcParams
rcParams['figure.figsize'] = 12, 8

#reading the csv file into the dataframe data and viewing top 5 rows
data = pd.read_csv("DMV_Written_Tests.csv")
data.head()

#viewing informatio about the dataframe data
data.info()

#made arrays to store data
scores = data[['DMV_Test_1', 'DMV_Test_2']].values
results = data['Results'].values

#best to visualize the data first to know where the descision boundary lies
#visualize the test scores
#the descision boundary will be linear obv

#create a scatter plot and color points based on class label
#points passed will be one color and the other will be points failed


passed = (results == 1).reshape(100,1)#col vector 100 rows and 1 col
failed = (results == 0).reshape(100,1)

#use seaborn to plot these vectors separately

ax = sns.scatterplot(x = scores[passed[:, 0], 0],
                    y = scores[passed[:, 0], 1],
                    marker = "^",
                    color = "green",
                    s = 60)


sns.scatterplot(x = scores[failed[:, 0], 0],
                    y = scores[failed[:, 0], 1],
                    marker = "X",
                    color = "red",
                    s = 60)

ax.set(xlabel = "DMV Written Test 1 Scores", ylabel = "DMV Written Test 2 Scores")
ax.legend(['Passed', 'Failed'])
plt.show();


def logistic_func(x):
    return 1/(1 + np.exp(-x) ) 

logistic_func(0)

def compute_cost(theta,x,y):
    m = len(y)
    y_pred = logistic_func(np.dot(x, theta))
    error = (y * np.log(y_pred)) + (1-y) * np.log(1-y_pred)
    cost = -1/m*sum(error)
    gradient = 1/m*np.dot(x.transpose(), (y_pred - y))
    return cost[0], gradient


#perform data standerdization
mean_scores = np.mean(scores, axis =0)
std_scores = np.std(scores, axis = 0)
scores = (scores - mean_scores)/std_scores

rows = scores.shape[0]# looks at the rows of feature matrix
cols = scores.shape[1]# vals of cols in data

X = np.append(np.ones((rows, 1)), scores, axis = 1)# append to scores on vertical axis
y = results.reshape(rows, 1)

theta_init = np.zeros((cols + 1, 1))
cost, gradient = compute_cost(theta_init, X, y)

print("cost at initialisation", cost)
print("gradients at init", gradient)


def gradient_descent(x, y, theta, alpha, iterations):
    costs = []
    for i in range(iterations):
        cost, gradient = compute_cost(theta, x, y)
        theta -= (alpha * gradient)
        costs.append(cost)
    return theta, costs


theta, costs = gradient_descent(X, y, theta_init, 1, 200)


print("theta after running gradient descent: ", theta)
print("resulting cost:", costs[-1])


plt.plot(costs)
plt.xlabel("Iterations")
plt.ylabel("$J(\Theta)$")
plt.title("Values of Cost Function over Iterations of Gradient")



ax = sns.scatterplot(x = X[passed[:, 0], 1],
                    y = X[passed[:, 0], 2],
                    marker = "^",
                    color = "green",
                    s = 60)


sns.scatterplot(x = X[failed[:, 0], 1],
                    y = X[failed[:, 0], 2],
                    marker = "X",
                    color = "red",
                    s = 60)

ax.legend(["passed","failed"])
ax.set(xlabel="DMV Written Test 1 Scores", ylabel="DMV Written Test 2 Scores")

x_boundary = np.array([np.min(X[:,1]), np.max(X[:,1])])
y_boundary = -(theta[0]+theta[1]*x_boundary)/theta[2]

sns.lineplot(x=x_boundary, y=y_boundary, color = "blue" )
plt.show();

def predict(theta, x):
    results = x.dot(theta)
    return results > 0


p = predict(theta, X)
print("Training accuraacy: ", sum(p==y)[0],"%")


test = np.array([50, 79])
test = (test - mean_scores)/std_scores
test = np.append(np.ones(1), test)
probability = logistic_func(test.dot(theta))
print("A person who scores 50 and 70 on their DMV test have a, ", 
      np.round(probability[0],2 ), " probab of passing")
